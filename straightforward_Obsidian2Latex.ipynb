{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use\n",
    "## Prerequisites\n",
    "1. Have Python 3 installed\n",
    "\n",
    "\n",
    "\n",
    "## Usage\n",
    "For each user-defined parameter, go to the [User Parameters](#user-parameters) section, wherein the 'PARS' dictionary is located.\n",
    "\n",
    "To set the paths for the .md file to be converted, change the `PARS['📂']['markdown-file']` and `PARS['📂']['tex-file']`.\n",
    "Then, just run all code blocks and VOILA!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages and helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from os.path import exists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_dict(D):\n",
    "    for key in D.keys():\n",
    "        if D[key] == '🟢':\n",
    "            D[key] = True\n",
    "        elif D[key] == '🔴':\n",
    "            D[key] = False\n",
    "    return D\n",
    "\n",
    "\n",
    "is_in_table_line = lambda x: x.startswith('|') and x.endswith('|')\n",
    "enum             = lambda x: enumerate(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants (to not be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID__TABLES__table_alignment__center = 0\n",
    "ID__TABLES__table_alignment__right  = 1\n",
    "ID__TABLES__table_alignment__middle = 2\n",
    "\n",
    "\n",
    "ID__TABLES__PACKAGE__longtblr = 0\n",
    "ID__TABLES__PACKAGE__tabularx = 1\n",
    "\n",
    "ID__CNV__TABLE_STARTED      = 0\n",
    "ID__CNV__TABLE_ENDED        = 1\n",
    "ID__CNV__IDENTICAL          = 2\n",
    "\n",
    "# ⚠ does not work for longtblr!\n",
    "CMD__TABLE__TABULARX__CENTERING = '\\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}'\n",
    "\n",
    "# For recognizing file names, section names, block names\n",
    "SPECIAL_CHARACTERS = ' %💬⚠💼🟢➕❓🔴✔🧑☺📁⚙🔒🟡🔲💊💡🤷‍♂️▶📧🔗🎾👨‍💻📞💭📖ℹ🤖🏢🧠🕒👇📚👉0-9'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files  = 'C:\\\\Users\\\\mario\\\\OneDrive - NTNU\\\\FILES\\\\workTips\\\\'\n",
    "path0       = path_files + 'AUTOMATIONS\\\\'\n",
    "\n",
    "\n",
    "PARS = conv_dict(dict({\n",
    "    '⚙': # SETTINGS\n",
    "        dict({\n",
    "            'TABLES':  \n",
    "                    conv_dict(dict({\n",
    "                                      'package': ID__TABLES__PACKAGE__longtblr,\n",
    "                           'hlines-to-all-rows': '🔴',\n",
    "                            'any-hlines-at-all': '🔴',\n",
    "                                    'alignment': [\n",
    "                                                    ID__TABLES__table_alignment__center,\n",
    "                                                    ID__TABLES__table_alignment__middle],\n",
    "                                    'rel-width': 1.2\n",
    "                    })),\n",
    "            'COMMENTS':\n",
    "                conv_dict(dict({\n",
    "                    'leave-empty-lines-as-is': '🟢'})) \n",
    "                }),\n",
    "    '📁':\n",
    "         dict({\n",
    "                'markdown-file': path_files+'Literature\\\\Theory\\\\Theory\\\\Reinforcement-Learning\\\\Equations\\\\update equations\\\\eq--expected update for state-action pair.md',  # Markdown (.md) file for conversion\n",
    "                     'tex-file': path_files+'Literature\\\\Theory\\\\Theory\\\\Reinforcement-Learning\\\\Equations\\\\update equations\\\\eq--expected update for state-action pair.tex',  # LateX (.tex) file (converted from the .md file)\n",
    "                        'vault': path_files\n",
    "            }),\n",
    "    'par':\n",
    "        dict({\n",
    "            'tabular-package':\n",
    "                            dict({\n",
    "                                       'names': ['longtblr', 'tabularx'],\n",
    "                                'before-lines': ['{colspec}']\n",
    "                            })\n",
    "        })\n",
    "}))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A spanning comment: %% this is ',\n",
       " 'SPARTA%% did you get that %% or not malaka?',\n",
       " 'what about %% three %% pp1 %% comments %% pp2 %% yall %%?',\n",
       " '',\n",
       " '%% this is a full comment line%%',\n",
       " 'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the markdown comment \n",
    "\n",
    "\n",
    "LINES = [\n",
    "    'A spanning comment: %% this is ',\n",
    "    'SPARTA%% did you get that %% or not malaka?',\n",
    "    'what about %% three %% pp1 %% comments %% pp2 %% yall %%?',\n",
    "    'This is a noncomment line',\n",
    "    '%% this is a full comment line%%',\n",
    "    'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%'\n",
    "]\n",
    "\n",
    "remove_markdown_comments(LINES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_loader():\n",
    "\n",
    "    packages_to_load = [\n",
    "        'hyperref',\n",
    "        'graphicx',\n",
    "        'amssymb',           # need more symbols\n",
    "        'titlesec',          # so that we can add more subsections (using 'paragraph')\n",
    "        'mathtools'\n",
    "         ]\n",
    "\n",
    "    tables_package = PARS['⚙']['TABLES']['package']\n",
    "    if tables_package == ID__TABLES__PACKAGE__longtblr:\n",
    "\n",
    "        packages_to_load.append('tabularray')\n",
    "        packages_to_load.append('longtable')\n",
    "\n",
    "    elif tables_package == ID__TABLES__PACKAGE__tabularx:\n",
    "        packages_to_load.append('tabularx')\n",
    "\n",
    "    return ['\\\\usepackage{'+x+'}' for x in packages_to_load]\n",
    "\n",
    "def symbol_replacement(S):\n",
    "    \n",
    "    SYMBOLS_TO_REPLACE = [\n",
    "        ['✔',       '\\\\checkmark',            1],\n",
    "        ['🟢',      '$\\\\\\\\blacklozenge$',    2],\n",
    "        ['🔴',      '\\\\\\maltese',            2],\n",
    "        ['➕',      '\\\\boxplus',             2],\n",
    "        ['🔗',      'LINK',                  1],\n",
    "        ['\\implies','\\Rightarrow',           1],\n",
    "        ['❓',       '?',                     1],\n",
    "        ['\\\\text',   '\\\\textnormal',         1],\n",
    "        ['💬',      'comments',             1],\n",
    "        ['⚠',      '!!!',                  1]\n",
    "        ]\n",
    "    \n",
    "    S_1 = []\n",
    "    for i, s in enum(S):\n",
    "        s1 = s\n",
    "        for symbol in SYMBOLS_TO_REPLACE:\n",
    "            if symbol[2] == 1:\n",
    "                s1 = s1.replace(symbol[0], symbol[1] + ' ')\n",
    "            elif symbol[2] == 2:\n",
    "                s1 = re.sub(symbol[0], symbol[1] + ' ', s1)\n",
    "        S_1.append(s1)\n",
    "\n",
    "    return S_1\n",
    "\n",
    "def replace_hyperlinks(S):\n",
    "    \n",
    "\n",
    "    # Anything that isn't a square closing bracket\n",
    "    name_regex = \"[^]]+\"\n",
    "    # http:// or https:// followed by anything but a closing paren\n",
    "    url_regex = \"http[s]?://[^)]+\"\n",
    "\n",
    "    markup_regex = '\\[({0})]\\(\\s*({1})\\s*\\)'.format(name_regex, url_regex)\n",
    "\n",
    "    S_1 = []\n",
    "    for s in S:\n",
    "        s1 = s \n",
    "\n",
    "        for match in re.findall(markup_regex, s1):\n",
    "            markdown_link = '[' + match[0] + '](' + match[1] + ')'\n",
    "            latex_link = \"\\\\href{\" + match[1] + \"}{\" + match[0] + \"}\"\n",
    "            s1 = s1.replace(markdown_link, latex_link)\n",
    "\n",
    "        S_1.append(s1)\n",
    "    \n",
    "    return S_1\n",
    "\n",
    "def identify__tables(S):\n",
    "\n",
    "    table_indexes = []\n",
    "    table_has_started = False\n",
    "    for i, l in enum(S):\n",
    "        lstr = l.lstrip().rstrip()\n",
    "        is_table_line = is_in_table_line(lstr)        \n",
    "        if is_table_line and (not table_has_started):\n",
    "            table_has_started = True\n",
    "            idx__table_start = i\n",
    "        # ⚠ NEVER add \"or (i == len(S)-1)\" to the condition below    \n",
    "        elif (not is_table_line and table_has_started):\n",
    "            table_has_started = False\n",
    "            idx__table_end = i\n",
    "            table_indexes.append(idx__table_start)\n",
    "            table_indexes.append(idx__table_end)\n",
    "\n",
    "\n",
    "    return table_indexes\n",
    "            \n",
    "def remove_markdown_comments(S):\n",
    "\n",
    "    # UNDER DEVELOPMENT!!!\n",
    "\n",
    "    comment_has_opened = False\n",
    "    comment_remains_open_for_line_i = False\n",
    "    line_number__of_comment_to_close = 0\n",
    "\n",
    "\n",
    "    for i, s in enum(S):\n",
    "        \n",
    "        occurences = [x.start() for x in re.finditer('%%', s)]\n",
    "        L = len(occurences)\n",
    "        if (L > 0):\n",
    "\n",
    "            if not comment_has_opened: comment_has_opened = True\n",
    "\n",
    "            if L % 2 == 0:\n",
    "                if not comment_remains_open_for_line_i:\n",
    "                    # we have no interline spanning comments\n",
    "                    replacements = [s[occurences[2*j]+2:occurences[2*j+1]] for j in range(int(L/2))]\n",
    "                    for repl in replacements:\n",
    "                        s = s.replace('%%'+ repl +'%% ','')    # ⚠WARNING-2: Bad programming (sloppy way to remove additional space)\n",
    "                        s = s.replace(' %%'+ repl +'%% ','')\n",
    "                        s = s.replace('%%'+ repl +'%%','')\n",
    "\n",
    "                    comment_has_opened = False\n",
    "                else:\n",
    "                    # 1. remove all content until first occurences[0]\n",
    "                    s0 = s\n",
    "                    s = s.replace(s[:occurences[0]+2], '')\n",
    "                    occurences = occurences[1:]\n",
    "                    L -= 1\n",
    "                    # Remove the rest\n",
    "                    replacements = [s0[occurences[2*j]+2:occurences[2*j+1]] for j in range(int((L-1)/2))]\n",
    "                    for repl in replacements:                    \n",
    "                        s = s.replace('%%'+ repl +'%% ','')    # ⚠WARNING-2: Bad programming (sloppy way to remove additional space)\n",
    "                        s = s.replace(' %%'+ repl +'%% ','')\n",
    "                        s = s.replace('%%'+ repl +'%%','')\n",
    "\n",
    "                    s = s.replace(' %%'+s0[occurences[-1]+2:],'')\n",
    "                    s = s.replace('%%'+s0[occurences[-1]+2:],'')                    \n",
    "                    # IMPLIED THAT: comment_remains_open_for_line_i = True\n",
    "\n",
    "            else:\n",
    "                if not comment_remains_open_for_line_i:\n",
    "                    # we have no interline spanning comments, BUT we are creating one\n",
    "                    s0 = s # need this, since in the following loop, len(s) changes, but the two lines after the loop operate based on the initial length\n",
    "                    replacements = [s[occurences[2*j]+2:occurences[2*j+1]] for j in range(int((L-1)/2))]\n",
    "                    for repl in replacements:                    \n",
    "                        s = s.replace('%%'+ repl +'%% ','')    # ⚠WARNING-2: Bad programming (sloppy way to remove additional space)\n",
    "                        s = s.replace(' %%'+ repl +'%% ','')\n",
    "                        s = s.replace('%%'+ repl +'%%','')\n",
    "\n",
    "                    s = s.replace(' %%'+s0[occurences[-1]+2:],'')\n",
    "                    s = s.replace('%%'+s0[occurences[-1]+2:],'')\n",
    "\n",
    "                    line_number__of_comment_to_close = i\n",
    "                    comment_remains_open_for_line_i = True\n",
    "                else:\n",
    "                    # 1. remove all content until occurences[0]\n",
    "                    s0 = s\n",
    "                    s = s.replace(s[:occurences[0]+2], '')\n",
    "                    occurences = occurences[1:]                    \n",
    "                    L -= 1\n",
    "                    replacements = [s[occurences[2*j]+2:occurences[2*j+1]] for j in range(int(L/2))]\n",
    "                    for repl in replacements:\n",
    "                        s = s.replace('%%'+ repl +'%% ','')    # ⚠WARNING-2: Bad programming (sloppy way to remove additional space)\n",
    "                        s = s.replace(' %%'+ repl +'%% ','')\n",
    "                        s = s.replace('%%'+ repl +'%%','')\n",
    "\n",
    "                    comment_remains_open_for_line_i = False\n",
    "\n",
    "\n",
    "        else:\n",
    "            if comment_remains_open_for_line_i:        \n",
    "                s = ''\n",
    "            \n",
    "        S[i] = s\n",
    "\n",
    "\n",
    "    return S\n",
    " \n",
    "def convert__tables(S):\n",
    "    '''\n",
    "    Converts tables    \n",
    "    '''\n",
    "\n",
    "    TABLE_SETTINGS = PARS['⚙']['TABLES']\n",
    "    package = TABLE_SETTINGS['package']\n",
    "    add_txt = ''\n",
    "    if (ID__TABLES__table_alignment__center in TABLE_SETTINGS['alignment']) \\\n",
    "        and package == ID__TABLES__PACKAGE__longtblr:\n",
    "        add_txt = '\\centering '\n",
    "\n",
    "\n",
    "    # After having found the table\n",
    "    ## We expect that the 1st line defines the columns\n",
    "\n",
    "    cols = S[0].split('|')\n",
    "    cols = [[x.lstrip().rstrip() for x in cols if len(x)>0 and x!='\\n']]\n",
    "\n",
    "    C = []\n",
    "    for s in S[2:]:\n",
    "        c = s.split('|')\n",
    "        c = [x.lstrip().rstrip() for x in c if len(x)>0 and x!='\\n']\n",
    "        C.append(c)\n",
    "\n",
    "    y = cols + C\n",
    "\n",
    "    # CONVERT\n",
    "    N_cols = len(cols[0])\n",
    "\n",
    "    latex_table = []\n",
    "    addText = ''\n",
    "    for i, c in enum(y):\n",
    "        c1 = [add_txt + x for x in c]\n",
    "        if i==0: \n",
    "            if TABLE_SETTINGS['any-hlines-at-all']:\n",
    "                addText = ' \\hline'\n",
    "        else:\n",
    "            if TABLE_SETTINGS['hlines-to-all-rows']:\n",
    "                addText = ' \\hline'\n",
    "        latex_table.append('    ' + \" & \".join(c1) + ' \\\\\\\\' + addText)\n",
    "\n",
    "    lbefore = []\n",
    "\n",
    "\n",
    "    if package == ID__TABLES__PACKAGE__tabularx:\n",
    "\n",
    "\n",
    "        PCKG_NAME = '{tabularx}'\n",
    "\n",
    "        if ID__TABLES__table_alignment__center in TABLE_SETTINGS['alignment']:\n",
    "            lbefore.append(CMD__TABLE__TABULARX__CENTERING)\n",
    "            colPrefix = 'Y'\n",
    "        else:\n",
    "            colPrefix = 'X'\n",
    "\n",
    "        if (ID__TABLES__table_alignment__middle in TABLE_SETTINGS['alignment']):\n",
    "            lbefore.append('\\\\renewcommand\\\\tabularxcolumn[1]{m{#1}}')\n",
    "\n",
    "        latex_before_table = lbefore + [\n",
    "            '\\\\begin{center}',\n",
    "            '\\\\begin'+PCKG_NAME+'{\\\\textwidth}{' + '|' + N_cols*(colPrefix+'|') + '}',\n",
    "            '   \\hline'\n",
    "        ]\n",
    "\n",
    "        latex_after_table = [\n",
    "            '   \\hline',\n",
    "            '\\end'+PCKG_NAME,\n",
    "            '\\end{center}'\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    elif package == ID__TABLES__PACKAGE__longtblr:\n",
    "\n",
    "        PCKG_NAME = '{longtblr}'\n",
    "\n",
    "        latex_before_table = [\n",
    "            '\\\\begin{center}',\n",
    "            '\\\\begin' + PCKG_NAME + '[',\n",
    "            'caption = {},',\n",
    "            'entry = {},',\n",
    "            'label = {},',\n",
    "            'note{a} = {},',\n",
    "            'note{$\\dag$} = {}]',\n",
    "            '   {colspec = {'+ N_cols*'X' +'}, width = ' + str(TABLE_SETTINGS['rel-width']) + '\\linewidth, hlines, rowhead = 2, rowfoot = 1}'\n",
    "            ]  \n",
    "\n",
    "        latex_after_table = [\n",
    "            '\\end' + PCKG_NAME,\n",
    "            '\\end{center}'\n",
    "        ]\n",
    "\n",
    "        add_hline_at_end = False # to be moved to user settings\n",
    "        if add_hline_at_end:\n",
    "            latex_after_table = '   \\hline' + latex_after_table\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('NOTHING CODED HERE!')\n",
    "\n",
    "\n",
    "    LATEX = latex_before_table + latex_table + latex_after_table\n",
    "\n",
    "\n",
    "    return LATEX\n",
    "\n",
    "def internal_links__identifier(S):\n",
    "\n",
    "    if not isinstance(S, list):\n",
    "        raise Exception('Input of the function must be a list of strings!')\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    pattern_sections = '\\[\\[([\\w-]+)\\#([\\w' + SPECIAL_CHARACTERS + '\\-]+)(\\|[\\w' + SPECIAL_CHARACTERS + '\\-]+)?\\]\\]'\n",
    "    pattern_blocks = '\\[\\[([\\w-]+)\\#\\^([\\w' + SPECIAL_CHARACTERS + '\\-]+)(\\|[\\w' + SPECIAL_CHARACTERS + '\\-]+)?\\]\\]'\n",
    "    \n",
    "    MATCHES = []\n",
    "    for i, s in enum(S):\n",
    "        match_sections = re.findall(pattern_sections, s)\n",
    "        match_blocks = re.findall(pattern_blocks, s)\n",
    "        if len(match_sections) != 0 or len(match_blocks) != 0:\n",
    "            MATCHES.append([i, match_sections, match_blocks])\n",
    "    \n",
    "    return MATCHES\n",
    "\n",
    "def internal_links__enforcer(S, sections_blocks, internal_links):\n",
    "\n",
    "    type_of_link = ['sec:', '']\n",
    "    type_of_link_obsidian = ['#', '#^']\n",
    "    sections = sections_blocks[0]\n",
    "    blocks = sections_blocks[1]\n",
    "    section_names = [x[1] for x in sections]\n",
    "    block_names = [x[1] for x in blocks]\n",
    "\n",
    "\n",
    "    # Just replace the labels, even though they are not being referenced                            ⚠WARNING--1\n",
    "    for block in blocks:\n",
    "        line_of_block, block_text = block\n",
    "        block_text_1 = '^' + block_text\n",
    "        S[line_of_block] = S[line_of_block].replace(block_text_1, ' \\label{' + block_text + '}')\n",
    "\n",
    "\n",
    "    for I in internal_links:\n",
    "        for iS in range(2):\n",
    "            Ii_sb = I[iS+1]\n",
    "            if len(Ii_sb) != 0:\n",
    "                \n",
    "                for i in Ii_sb:\n",
    "                    section_i = Ii_sb[0][1]\n",
    "                    idx = [j for j in range(len(sections_blocks[iS])) if sections_blocks[iS][j][1] == section_i]\n",
    "                    if len(idx)>0: \n",
    "                        idx=idx[0]\n",
    "\n",
    "                        label = type_of_link[iS] + section_i.replace(' ', '-')\n",
    "                        label_of_source = ' \\label{' + label + '}'\n",
    "                        hyperref_text = Ii_sb[0][-1].replace('|', '')\n",
    "                        if len(hyperref_text) != 0:\n",
    "                            hyperref_text = '{' + hyperref_text + '}'\n",
    "                        else:\n",
    "                            hyperref_text = '{' + 'ADD_NAME' + '}'\n",
    "\n",
    "                        if not label_of_source in S[sections_blocks[iS][idx][0]]:\n",
    "                            # Has not already been replaced\n",
    "\n",
    "                            label__in_line = S[sections_blocks[iS][idx][0]].replace('\\n', '')\n",
    "                            add__S_repl = ' \\label{' + type_of_link[iS] + section_i.replace(' ', '-') + '}'\n",
    "\n",
    "                            # Perform replacements on the label\n",
    "                            if iS==0:\n",
    "                                S[sections_blocks[iS][idx][0]] = label__in_line + add__S_repl\n",
    "                            else:\n",
    "                                S[sections_blocks[iS][idx][0]] = label__in_line.replace('^' + label, '') + add__S_repl\n",
    "\n",
    "\n",
    "                        hyperref = '\\hyperref[' + label + ']' + hyperref_text\n",
    "\n",
    "                        obsidian_hyperref = '[[' + Ii_sb[0][0] + type_of_link_obsidian[iS] + Ii_sb[0][1] + Ii_sb[0][2] + ']]'\n",
    "                        S[I[0]] = S[I[0]].replace(obsidian_hyperref, hyperref)\n",
    "\n",
    "\n",
    "                    # else:\n",
    "                    #     # Just replace the labels, even though they are not being referenced\n",
    "                    #     for block in blocks:\n",
    "                    #         line_of_block, block_text = block\n",
    "                    #         block_text_1 = '^' + block_text\n",
    "                    #         S[line_of_block] = S[line_of_block].replace(block_text_1, ' \\label{' + block_text + '}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return S\n",
    "\n",
    "def embedded_references_recognizer(S):\n",
    "\n",
    "\n",
    "    all_chars = '\\w' + SPECIAL_CHARACTERS + '\\-'\n",
    "    if not isinstance(S, list):\n",
    "        raise Exception('Input of the function must be a list of strings!')\n",
    "        return np.nan\n",
    "\n",
    "    pattern_embedded = '!\\[\\[([\\.'+all_chars+']+)(\\|[\\w' + SPECIAL_CHARACTERS + '\\-]+)?\\]\\]'\n",
    "    MATCHES = []\n",
    "    for i, s in enum(S):\n",
    "        match_pattern_embedded = re.findall(pattern_embedded, s)\n",
    "        if len(match_pattern_embedded) != 0:\n",
    "            MATCHES.append([i, match_pattern_embedded])\n",
    "            # path-finder\n",
    "\n",
    "    \n",
    "    return MATCHES\n",
    "\n",
    "def unfold_embedded_notes(S, md__files_embedded):\n",
    "\n",
    "    '''\n",
    "    Unfolds the content of embedded notes.\n",
    "\n",
    "    ---------\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "        1. S (List): the content of the note (including so-far conversions)\n",
    "        2. md__files_embedded(List): a list that contains all embedded references. It is needed to ensure that we do not reach an infinite loop of unfolding notes\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    if not isinstance(md__files_embedded, list):\n",
    "        raise Exception('md__files_embedded variable must be of type list!')\n",
    "\n",
    "    ss1 = embedded_references_recognizer(S)\n",
    "\n",
    "    file_types = ['.png', '.pdf', 'jpg']\n",
    "\n",
    "    for ln in ss1:\n",
    "        line_number = ln[0]\n",
    "        line_embeds = ln[1]\n",
    "        for line_embed in line_embeds:\n",
    "\n",
    "            has_extension = False\n",
    "\n",
    "            embedded_ref = line_embed[0]\n",
    "            markdown_ref = '![[' + line_embed[0] + line_embed[1]    +    ']]'\n",
    "            for file_type in file_types:\n",
    "                if file_type in embedded_ref:\n",
    "                    has_extension = True\n",
    "                    # break\n",
    "\n",
    "            if not has_extension: \n",
    "                # means that it is a .md file, which we need to unfold\n",
    "\n",
    "                if not embedded_ref in md__files_embedded:\n",
    "                    # Unfold this note ONLY when it hasn't already been unfolded\n",
    "                    md__files_embedded.append(embedded_ref)\n",
    "\n",
    "                    embedded_ref += '.md'\n",
    "\n",
    "                    path = embedded_references_path_finder(embedded_ref)\n",
    "\n",
    "                    if len(path) == 0:\n",
    "                        raise Exception('File: ' + embedded_ref + ' cannot be found in ' + PARS['📁']['vault'])\n",
    "\n",
    "                    try:\n",
    "                        with open(path, 'r', encoding='utf8') as f:\n",
    "                            content__embedded_notes = f.readlines()\n",
    "                    except:\n",
    "                        raise Exception('File: ' + embedded_ref + ' cannot be found in ' + PARS['📁']['vault'])\n",
    "\n",
    "\n",
    "                    S[line_number] = S[line_number].replace(markdown_ref, ''.join(content__embedded_notes))\n",
    " \n",
    "    return S, md__files_embedded\n",
    "\n",
    "def images_converter(images):\n",
    "\n",
    "    # NOTES:\n",
    "    # --- \", height=0.5\\\\textheight\" addition causes the aspect ratio to break\n",
    "\n",
    "    TO_PRINT = []\n",
    "\n",
    "    for IM in images:\n",
    "        path_img = '\"' + IM[1].replace('\\\\', '/') + '\"'\n",
    "        label_img = IM[1].split('\\\\')[-1]\n",
    "        caption_short = 'Caption short'\n",
    "        caption_long = 'Caption long'\n",
    "\n",
    "        TO_PRINT.append(' \\n'.join([\n",
    "        '\\\\begin{figure}',\n",
    "        '\t\\centering',\n",
    "        '\t\\includegraphics[width=0.7\\linewidth]'+\\\n",
    "            '{\"'+path_img+'\"}',\n",
    "        '\t\\caption['+caption_short+']{'+caption_long+'}',\n",
    "        '\t\\label{fig:'+label_img+'}',\n",
    "        '\\end{figure}']))\n",
    "\n",
    "    return TO_PRINT\n",
    "\n",
    "def embedded_references_path_finder(u):\n",
    "\n",
    "    files = []\n",
    "    # for folder, subfolders, files in os.walk(PARS['📁']['vault']):\n",
    "    #    for f in files:\n",
    "    #     if f.endswith('.md'): files_md.append(f)\n",
    "    os.chdir(PARS['📁']['vault'])\n",
    "    for root, dirs, files in os.walk(PARS['📁']['vault']):\n",
    "        if u in files: return os.path.join(root,u)\n",
    "    return ''\n",
    "\n",
    "def bullet_list_converter(S):\n",
    "\n",
    "    S = ''.join(S)\n",
    "\n",
    "    latex = \"\"\n",
    "    lines = S.split(\"\\n\")\n",
    "    indent = 0\n",
    "    intent_list_type = []\n",
    "    # tab_1 = \" \"*4\n",
    "    tab_1 = \"\\t\"\n",
    "    Lt = len(tab_1)\n",
    "    beg_item = \"\\\\begin{itemize}\\n\"\n",
    "    beg_enum = \"\\\\begin{enumerate}\\n\"\n",
    "\n",
    "    end_type = [\"\\\\end{itemize}\\n\", \"\\\\end{enumerate}\\n\"]\n",
    "\n",
    "    first_itemize = False\n",
    "    number_list = 1\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(tab_1* indent + \"- \") or line.startswith(tab_1* indent + \"* \"):\n",
    "            \n",
    "            intent_list_type.append([indent, 0])\n",
    "            if not first_itemize: \n",
    "                first_itemize = True\n",
    "                list_closed = False\n",
    "                beg_item_i = beg_item\n",
    "                Li = 1\n",
    "            else:\n",
    "                beg_item_i = \"\"\n",
    "                Li = 1\n",
    "            latex += tab_1* (indent)*(len(beg_item_i)>0) + beg_item_i +  tab_1* (indent+1) + \"\\\\item \" + line[2 + indent*Lt:].strip() + \"\\n\"\n",
    "        elif line.startswith(tab_1* (indent + 1) + \"- \") or line.startswith(tab_1* (indent + 1) + \"* \"):\n",
    "            latex += tab_1 * (indent+1) + beg_item\n",
    "            indent += 1\n",
    "            intent_list_type.append([indent, 0])\n",
    "            latex += tab_1 * (indent+1) + \"\\\\item \" + line[2 + (indent+0)*Lt:].strip() + \"\\n\"\n",
    "        elif line.startswith(tab_1* (indent - 1) + \"- \") or line.startswith(tab_1* (indent - 1) + \"* \") and indent > 0:\n",
    "            indent -= 1\n",
    "            intent_list_type.append([indent, 0])\n",
    "            latex += tab_1 * indent + \"\\\\end{itemize}\\n\"\n",
    "            latex += tab_1 * indent + \"\\\\item \" + line[2 + (indent-1)*Lt:].strip() + \"\\n\"\n",
    "        \n",
    "        elif line.startswith(tab_1* (indent - 2) + \"- \") or line.startswith(tab_1* (indent - 2) + \"* \") and indent > 0:\n",
    "            # UNDER DEV ---> need to close previous lists\n",
    "            indent -= 2\n",
    "            intent_list_type.append([indent, 0])\n",
    "            latex += tab_1 * (indent-1) + \"\\\\end{itemize}\\n\"\n",
    "            latex += tab_1 * (indent-1) + \"\\\\item \" + line[2 + (indent-2)*Lt:].strip() + \"\\n\"\n",
    "        \n",
    "        elif line.startswith(tab_1* (indent - 3) + \"- \") or line.startswith(tab_1* (indent - 3) + \"* \") and indent > 0:\n",
    "            \n",
    "            # UNDER DEV ---> need to close previous lists\n",
    "            indent -= 3\n",
    "            intent_list_type.append([indent, 0])\n",
    "            latex += tab_1 * (indent-2) + \"\\\\end{itemize}\\n\"\n",
    "            latex += tab_1 * (indent-2) + \"\\\\item \" + line[2 + (indent-3)*Lt:].strip() + \"\\n\"\n",
    "\n",
    "\n",
    "        elif line.startswith(tab_1* indent + str(number_list) + \". \"):\n",
    "            intent_list_type.append([indent, 1])\n",
    "            latex += tab_1* (indent) +  tab_1* (indent+1) + \"\\\\item \" + line[2 + indent*Lt:].strip() + \"\\n\"\n",
    "            number_list += 1\n",
    "        \n",
    "        elif line.startswith(tab_1* (indent+1) + str(number_list) + \". \"):\n",
    "            latex += tab_1 * (indent+1) + beg_enum\n",
    "            indent += 1\n",
    "            intent_list_type.append([indent, 1])\n",
    "            latex += tab_1 * (indent+1) + \"\\\\item \" + line[2 + (indent+0)*Lt:].strip() + \"\\n\"\n",
    "            number_list += 1\n",
    "\n",
    "        else:\n",
    "            \n",
    "            while (indent > -1) and first_itemize:\n",
    "                \n",
    "                latex += tab_1 * indent +  end_type[[xx[1] for xx in intent_list_type if xx[0]==indent][0]]  \n",
    "                indent -= 1\n",
    "\n",
    "            latex += line + \"\\n\"\n",
    "            first_itemize = False\n",
    "            list_closed = True\n",
    "            indent = 0\n",
    "            number_list = 1\n",
    "            intent_list_type = []\n",
    "\n",
    "\n",
    "    if not list_closed: \n",
    "        # in case that the Lines end abruptly before getting the chance to close the list (rare)\n",
    "        while (indent > -1) and first_itemize:\n",
    "            \n",
    "            latex += tab_1 * indent + end_type[intent_list_type[indent][0]]\n",
    "            indent -= 1\n",
    "\n",
    "    return latex.split(\"\\n\")\n",
    "\n",
    "PATHS = PARS['📁']\n",
    "\n",
    "with open(PATHS['markdown-file'], 'r', encoding='utf8') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "content = remove_markdown_comments(content)\n",
    "\n",
    "# UNFOLD EMBEDDED NOTES ============================================================================================================================================\n",
    "md__files_embedded_prev0 = []\n",
    "md__files_embedded_prev = md__files_embedded_prev0.copy()\n",
    "[content, md__files_embedded_new] = unfold_embedded_notes(content, md__files_embedded_prev)\n",
    "\n",
    "while md__files_embedded_prev0 != md__files_embedded_new:\n",
    "    md__files_embedded_prev0 = md__files_embedded_new.copy()\n",
    "    md__files_embedded_prev = md__files_embedded_prev0.copy()\n",
    "    [content, md__files_embedded_new] = unfold_embedded_notes(content, md__files_embedded_prev)\n",
    "\n",
    "# ======================================================================================================================================================================\n",
    "\n",
    "\n",
    "content = bullet_list_converter(content)\n",
    "\n",
    "\n",
    "# Replace headers and map sections \\==================================================\n",
    "Lc = len(content)-1\n",
    "sections = []\n",
    "for i in range(Lc+1):\n",
    "    # ⚠ The sequence of replacements matters: \n",
    "    # ---- replace the lowest-level subsections first\n",
    "    content_00 = content[i]\n",
    "\n",
    "    content_0 = content[i]\n",
    "    content[i] = re.sub(r'#### (.*)', r'\\\\paragraph{\\1}', content[i].replace('%%', ''))\n",
    "    if content[i] != content_0:\n",
    "        sections.append([i, content_0.replace('#### ', '').replace('\\n', '')])\n",
    "\n",
    "    content_0 = content[i]\n",
    "    content[i] = re.sub(r'### (.*)', r'\\\\subsubsection{\\1}', content[i].replace('%%', ''))\n",
    "    if content[i] != content_0:\n",
    "        sections.append([i, content_0.replace('### ', '').replace('\\n', '')])\n",
    "\n",
    "    content_0 = content[i]\n",
    "    content[i] = re.sub(r'## (.*)', r'\\\\subsection{\\1}', content[i].replace('%%', ''))\n",
    "    if content[i] != content_0:\n",
    "        sections.append([i, content_0.replace('## ', '').replace('\\n', '')])\n",
    "\n",
    "    content_0 = content[i]\n",
    "    content[i] = re.sub(r'# (.*)', r'\\\\section{\\1}', content[i].replace('%%', ''))\n",
    "    if content[i] != content_0:\n",
    "        sections.append([i, content_0.replace('# ', '').replace('\\n', '')])\n",
    "\n",
    "# \\==================================================\\==================================================\n",
    "\n",
    "# find reference blocks \\==================================================\n",
    "#---1. they have to be at the end of the sentence (i.e. before \"\\n\")\n",
    "blocks = []\n",
    "for i in range(Lc+1):\n",
    "    s = content[i].replace('\\n', '')\n",
    "    pattern = r\"\\^[\\w\\-]*$\"\n",
    "    link_label = re.findall(pattern, s)\n",
    "    if len(link_label) > 0:\n",
    "        blocks.append([i, link_label[0].replace('^', '')])    \n",
    "# \\==================================================\n",
    "\n",
    "\n",
    "internal_links = internal_links__identifier(content)\n",
    "content = internal_links__enforcer(content, [sections, blocks], internal_links)\n",
    "\n",
    "\n",
    "# Convert figures \\==================================================\n",
    "\n",
    "embeded_refs = embedded_references_recognizer(content)\n",
    "\n",
    "# ➕ add more image refs\n",
    "# replace \"content[line_number]\" accordingly and see the result\n",
    "\n",
    "for i, ln in enum(embeded_refs):\n",
    "\n",
    "    line_number = ln[0]\n",
    "    line_refs = ln[1]\n",
    "    for lnrf in line_refs:\n",
    "\n",
    "        # print(embedded_references_path_finder(lnrf[0]))\n",
    "        converted_image_text = images_converter([[line_number, embedded_references_path_finder(lnrf[0])]])\n",
    "        for img_txt_cnv in converted_image_text:\n",
    "            if ('.png' in lnrf[0] or '.jpg' in lnrf[0]) and (lnrf[1].replace('|','')).isnumeric():\n",
    "                content[line_number] = content[line_number].replace('![[' + lnrf[0] + lnrf[1] + ']]', img_txt_cnv)\n",
    "            else:\n",
    "                content[line_number] = content[line_number].replace('![[' + lnrf[0] + ']]', img_txt_cnv)\n",
    "\n",
    "\n",
    "# \\==================================================\n",
    "\n",
    "\n",
    "IDX__TABLES = [0]\n",
    "TYPE_OF_CNV = [ID__CNV__IDENTICAL]\n",
    "tmp1 = identify__tables(content)\n",
    "tmp2 = [ID__CNV__TABLE_STARTED for _ in tmp1]\n",
    "tmp2[1::2] = [ID__CNV__IDENTICAL for _ in tmp1[1::2]]\n",
    "IDX__TABLES += tmp1\n",
    "TYPE_OF_CNV += tmp2\n",
    "\n",
    "Lc = len(content)-1\n",
    "if IDX__TABLES[-1] < Lc: \n",
    "    IDX__TABLES.append(Lc)\n",
    "    TYPE_OF_CNV.append(ID__CNV__IDENTICAL)\n",
    "\n",
    "LATEX_TABLES = []\n",
    "for i in range(int(len(tmp1)/2)):\n",
    "    LATEX_TABLES.append(convert__tables(content[tmp1[2*i]:tmp1[2*i+1]]))\n",
    "\n",
    "\n",
    "# for i, L in enum(content):\n",
    "\n",
    "#     for idx_table in IDX__TABLES:\n",
    "#         LATEX_TABLES.append(convert__tables(content[idx_table[0]:idx_table[1]]))\n",
    "\n",
    "\n",
    "LATEX = []\n",
    "i0 = IDX__TABLES[0]\n",
    "i_tables = 0\n",
    "for j, i in enum(IDX__TABLES[1:]):\n",
    "    if TYPE_OF_CNV[j] == ID__CNV__IDENTICAL:\n",
    "        LATEX += content[i0:i]\n",
    "    elif TYPE_OF_CNV[j] == ID__CNV__TABLE_STARTED:\n",
    "        LATEX += LATEX_TABLES[i_tables]\n",
    "        i_tables += 1\n",
    "    \n",
    "    i0 = i\n",
    "    \n",
    "LATEX = symbol_replacement(LATEX)   \n",
    "LATEX = replace_hyperlinks(LATEX)\n",
    "\n",
    "PREAMBLE = ['\\documentclass{article}'] + package_loader() + ['\\n'*2] + ['\\setcounter{secnumdepth}{4}'] + ['\\\\begin{document}']\n",
    "\n",
    "\n",
    "LATEX = PREAMBLE + LATEX + ['\\end{document}']\n",
    "with open(PATHS['tex-file'], 'w', encoding='utf8') as f:\n",
    "    for l in LATEX:\n",
    "        if not l.endswith('\\n'): l+='\\n'\n",
    "        f.write(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a noncomment line',\n",
       " '%% this is a full comment line%%',\n",
       " 'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the markdown comment \n",
    "\n",
    "\n",
    "LINES = [\n",
    "    'This is a noncomment line',\n",
    "    '%% this is a full comment line%%',\n",
    "    'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%'\n",
    "]\n",
    "\n",
    "remove_markdown_comments(LINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a noncomment line',\n",
       " '%% this is a full comment line%%',\n",
       " 'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the markdown comment \n",
    "\n",
    "\n",
    "LINES = [\n",
    "    'This is a noncomment line',\n",
    "    '%% this is a full comment line%%',\n",
    "    'Starts: %% This is a double comment line%% you motherfucker %% Zionists? %%'\n",
    "]\n",
    "\n",
    "remove_markdown_comments(LINES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugginng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [('Pasted image 20221127213454.png', '|500')]]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_references_recognizer(['This is ![[Pasted image 20221127213454.png|500]]'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chars = '\\w' + SPECIAL_CHARACTERS + '\\[\\]'\n",
    "\n",
    "pattern = r\"\\[([^\\]]+)\\]\"\n",
    "regexMdLinks = '/\\[([^\\[]+)\\](\\(.*\\))'\n",
    "s = '[some example]' \n",
    "s = '[Could not install packages due to an OSError: WinError 5 Access is denied](https://stackoverflow.com/questions/73339138/could-not-install-packages-due-to-an-oserror-winerror-5-access-is-denied)' \n",
    "match = re.findall(regexMdLinks, s)\n",
    "match\n",
    "\n",
    "\n",
    "# pattern = r\"\\[([^\\]]+)\\]\\(([^\\)]+)\\)\"\n",
    "\n",
    "# pattern = r\"\\[([^\\]]+)\\]\\(([^\\)]+)\\)\"\n",
    "# pattern = r\"\\[([^\\[\\]]+)\\]\\(([^\\(\\)]+)\\)\"\n",
    "# pattern = r\"\\[([^\\[\\]]+)\\]\\(([^\\(\\)]+)\\)\"\n",
    "\n",
    "\n",
    "# s = 'example with [linking a website](https://stackoverflow.com/questions/73339138/could-not-install-packages-due-to-an-oserror-winerror-5-access-is-denied)' \n",
    "\n",
    "# match = re.findall(pattern, s)\n",
    "# match\n",
    "\n",
    "# import re\n",
    "\n",
    "# pattern = r\"\\[([^\\[\\]]+)\\]\\(([^\\(\\)]+)\\)\"\n",
    "\n",
    "# text = r\"[some sentence with [brackets] or (parentheses) inside it](some website)\"\n",
    "\n",
    "# match = re.findall(pattern, text)\n",
    "# match\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Could not install packages due to an OSError: WinError 5 Access is denied', 'https://stackoverflow.com/questions/73339138/could-not-install-packages-due-to-an-oserror-winerror-5-access-is-denied')\n",
      "\\href{https://stackoverflow.com/questions/73339138/could-not-install-packages-due-to-an-oserror-winerror-5-access-is-denied}{Could not install packages due to an OSError: WinError 5 Access is denied}\n"
     ]
    }
   ],
   "source": [
    "# Anything that isn't a square closing bracket\n",
    "name_regex = \"[^]]+\"\n",
    "# http:// or https:// followed by anything but a closing paren\n",
    "url_regex = \"http[s]?://[^)]+\"\n",
    "text = '[Could not install packages due to an OSError: WinError 5 Access is denied](https://stackoverflow.com/questions/73339138/could-not-install-packages-due-to-an-oserror-winerror-5-access-is-denied)' \n",
    "\n",
    "markup_regex = '\\[({0})]\\(\\s*({1})\\s*\\)'.format(name_regex, url_regex)\n",
    "\n",
    "\n",
    "for match in re.findall(markup_regex, text):\n",
    "    print(match)\n",
    "    markdown_link = '[' + match[0] + '](' + match[1] + ')'\n",
    "    latex_link = \"\\\\href{\" + match[1] + \"}{\" + match[0] + \"}\"\n",
    "    print(text.replace(markdown_link, latex_link))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal links/crossrefs\n",
    "\n",
    "Using this format:\n",
    "\n",
    "\\section{Hello World}\n",
    "\\label{sec:hello}\n",
    "\n",
    "\n",
    "\\hyperref[sec:hello]{Word of text}\n",
    "\n",
    "\n",
    "### Strategy\n",
    "1. Add the label with the same name as in the Obsidian note. Add it just using \"\\n \\label{sec:label}\" instead of creating a new line\n",
    "2. Map the sections and blocks so that we can correspond them easily\n",
    "\n",
    "\n",
    "\n",
    "## Limitations\n",
    "\n",
    "### Hyperlinks\n",
    "- The pattern does not take account for the cases wherein there's more brackets inside the brackets\n",
    "\n",
    "\n",
    "### Cannot understand Windows emojis\n",
    "\n",
    "--> Use [this list of symbols](https://milde.users.sourceforge.net/LUCR/Math/mathpackages/amssymb-symbols.pdf) instead and the `\\usepackage{amssymb}` command\n",
    "\n",
    "\n",
    "\n",
    "## Programming mistakes/weaknesses in the code\n",
    "1. Redundant replacement in: \"⚠WARNING--1\" (search for it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa67766ae418968f0c59ac0eb4df618cb98e2442e22a4762b28c70784bead217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
